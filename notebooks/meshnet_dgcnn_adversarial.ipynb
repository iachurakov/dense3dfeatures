{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea01814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import sys\n",
    "# Path to workspace\n",
    "sys.path.insert(0, '/workspace/dense-self-supervised-representation-learning-for-3D-shapes/')\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import k3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2435b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/neptune/new/internal/utils/git.py:35: UserWarning: GitPython could not be initialized\n",
      "  warnings.warn(\"GitPython could not be initialized\")\n",
      "/opt/conda/lib/python3.8/site-packages/neptune/new/internal/utils/git.py:35: UserWarning: GitPython could not be initialized\n",
      "  warnings.warn(\"GitPython could not be initialized\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/igor3661/crossmodal/e/CROSS-51\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "import neptune.new as neptune\n",
    "from workspace.utils.train_loop import *\n",
    "\n",
    "params = {\n",
    "    'name': 'meshnet_dgcnn_modelnet',\n",
    "    'dataset': 'modelnet',\n",
    "    'batch_size': 10,\n",
    "    'tau': 0.07,\n",
    "    'n_output': 512,\n",
    "    'result_dim': 128,\n",
    "    'hidden_dim': 256,\n",
    "    'total_epochs': 100,\n",
    "    'lr': 2e-4,\n",
    "    'betas': (0.5, 0.999),\n",
    "    'weight_decay': 1e-5,\n",
    "    'save_every': 20,\n",
    "    'weights_root': 'weights/'\n",
    "}\n",
    "\n",
    "# tags\n",
    "tags = ['modelnet', 'meshnet', 'dgcnn', 'translation', 'mse']\n",
    "\n",
    "logger = neptune.init(\n",
    "    project=\"igor3661/crossmodal\",\n",
    "    name=params['name'],\n",
    "    tags=tags,\n",
    "    api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcG'\\\n",
    "              'lfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiN'\\\n",
    "              'zcxMGNkOS04ZjU3LTRmNDMtOWFjMS1kNDNkZDZlNDI4YWYifQ==',\n",
    ")  # your credentials\n",
    "\n",
    "\n",
    "logger['parameters'] = params\n",
    "\n",
    "device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdaf010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from workspace.crossmodal.utils.meshnet_preprop import *\n",
    "\n",
    "class MeshnetDataset(Dataset):\n",
    "    def __init__(self, data_path, rotation=None, jitter=None):\n",
    "        super().__init__()\n",
    "        self.rotation = rotation\n",
    "        self.jitter = jitter\n",
    "        self.file = h5py.File(data_path, 'r')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        faces = self.file['faces'][index][:].reshape(-1, 3)\n",
    "        vertices = self.file['vertices'][index][:].reshape(-1, 3)\n",
    "        \n",
    "        if self.rotation is not None:\n",
    "            vertices = self.rotation(vertices)\n",
    "        \n",
    "        features, neighbors = process_mesh(faces, vertices)\n",
    "        \n",
    "\n",
    "        features = torch.from_numpy(features).float()\n",
    "        neighbors = torch.from_numpy(neighbors).long()\n",
    "\n",
    "        features = torch.permute(features, (1, 0))\n",
    "        centers, corners, normals = features[:3], features[3:12], features[12:]\n",
    "        \n",
    "        if self.jitter is not None:\n",
    "            centers = self.jitter(centers).float()\n",
    "        \n",
    "        corners = corners - torch.cat([centers, centers, centers], 0).float()\n",
    "\n",
    "        return centers, corners, normals, neighbors #, normals\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.file['points'].shape[0]\n",
    "    \n",
    "    \n",
    "class PointDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.file = h5py.File(data_path, 'r')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        points = self.file['points'][index][:]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            points = self.transform(points)\n",
    "\n",
    "        points = torch.from_numpy(points).float()\n",
    "        points = torch.permute(points, (1, 0))\n",
    "        return points\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.file['points'].shape[0]\n",
    "    \n",
    "\n",
    "class DoubleDataset(Dataset):\n",
    "    def __init__(self, kwargs):\n",
    "        super().__init__()\n",
    "        self.mesh = MeshnetDataset(**kwargs['mesh'])\n",
    "        self.point = PointDataset(**kwargs['point'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        face_index = torch.from_numpy(self.mesh.file['face_index'][idx][:]).long()\n",
    "        return (*self.mesh.__getitem__(idx), self.point.__getitem__(idx), face_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.mesh.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ad3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workspace.datasets.transforms import *\n",
    "\n",
    "\n",
    "train_kwargs = {\n",
    "    'mesh': {\n",
    "        'data_path': 'modelnet/modelnet_train_1024.h5',\n",
    "        'rotation': RandomRotation(low=-45, high=45, axis='xyz'),\n",
    "        'jitter': RandomJitter(std=0.01, clip_bound=0.05)\n",
    "    },\n",
    "    'point': {\n",
    "        'data_path': 'modelnet/modelnet_train_1024.h5',\n",
    "        'transform': Compose(\n",
    "            RandomRotation(low=-45, high=45, axis='xyz'),\n",
    "            RandomJitter(std=0.01, clip_bound=0.05)\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "test_kwargs = {\n",
    "    'mesh': {\n",
    "        'data_path': 'modelnet/modelnet_test_1024.h5',\n",
    "        'rotation': RandomRotation(low=-45, high=45, axis='xyz'),\n",
    "        'jitter': RandomJitter(std=0.01, clip_bound=0.05)\n",
    "    },\n",
    "    'point': {\n",
    "        'data_path': 'modelnet/modelnet_test_1024.h5',\n",
    "        'transform': Compose(\n",
    "            RandomRotation(low=-45, high=45, axis='xyz'),\n",
    "            RandomJitter(std=0.01, clip_bound=0.05)\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "train_data = DoubleDataset(train_kwargs)\n",
    "test_data = DoubleDataset(test_kwargs)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=params['batch_size'],\n",
    "    num_workers=10,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=params['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "364853ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(torch.nn.Module):\n",
    "    def __init__(self, *dims):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, data):\n",
    "        return data.transpose(*self.dims)\n",
    "    \n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, model, model_output_dim, result_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.head = torch.nn.Sequential(\n",
    "            Transpose(1, 2),\n",
    "            torch.nn.Linear(model_output_dim, hidden_dim),\n",
    "            Transpose(1, 2),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            Transpose(1, 2),\n",
    "            torch.nn.Linear(hidden_dim, result_dim),\n",
    "            Transpose(1, 2),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, data):\n",
    "        return self.head(self.model.forward_features(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7076d3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from workspace.models.meshnet import MeshNet\n",
    "from workspace.models.dgcnn import DGCNN\n",
    "\n",
    "meshnet = MeshNet(n_patches=5)\n",
    "dgcnn = DGCNN(n_patches=5)\n",
    "\n",
    "mesh_model = Model(\n",
    "    meshnet,\n",
    "    model_output_dim=params['n_output'],\n",
    "    hidden_dim=params['hidden_dim'],\n",
    "    result_dim=params['result_dim']\n",
    ").to(device).eval()\n",
    "\n",
    "point_model = Model(\n",
    "    dgcnn,\n",
    "    model_output_dim=params['n_output'],\n",
    "    hidden_dim=params['hidden_dim'],\n",
    "    result_dim=params['result_dim']\n",
    ").to(device).eval()\n",
    "\n",
    "mesh_model.load_state_dict(torch.load('weights/CROSS-32/100epoch.pt'))\n",
    "point_model.load_state_dict(torch.load('weights/CROSS-33/100epoch.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c922b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector(torch.nn.Module):\n",
    "    def __init__(self, model_output_dim):\n",
    "        super().__init__()\n",
    "        self.head = torch.nn.Sequential(\n",
    "                Transpose(1, 2),\n",
    "                torch.nn.Linear(model_output_dim, model_output_dim * 2),\n",
    "                Transpose(1, 2),\n",
    "                torch.nn.BatchNorm1d(model_output_dim * 2),\n",
    "                torch.nn.ReLU(),\n",
    "                Transpose(1, 2),\n",
    "                torch.nn.Linear(model_output_dim * 2, model_output_dim),\n",
    "                Transpose(1, 2),\n",
    "            )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.head(data)\n",
    "    \n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, model_output_dim):\n",
    "        super().__init__()\n",
    "        self.head = torch.nn.Sequential(\n",
    "                Transpose(1, 2),\n",
    "                torch.nn.Linear(model_output_dim, model_output_dim // 2),\n",
    "                Transpose(1, 2),\n",
    "                torch.nn.BatchNorm1d(model_output_dim // 2),\n",
    "                torch.nn.ReLU(),\n",
    "                Transpose(1, 2),\n",
    "                torch.nn.Linear(model_output_dim // 2, 1),\n",
    "                Transpose(1, 2),\n",
    "            )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.head(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9693f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "projMP = Projector(params['n_output']).to(device)\n",
    "projPM = Projector(params['n_output']).to(device)\n",
    "discM = Discriminator(params['n_output']).to(device)\n",
    "discP = Discriminator(params['n_output']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fe135d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_L1 = torch.nn.L1Loss()\n",
    "\n",
    "def LOSS_D(real, fake):\n",
    "    return (torch.mean((real - 1) ** 2) + torch.mean(fake ** 2))\n",
    "\n",
    "def LOSS_G(fake):\n",
    "    return  torch.mean((fake - 1) ** 2)\n",
    "\n",
    "optimizer_MP = torch.optim.Adam(\n",
    "    params=projMP.parameters(), lr=params['lr'], betas=params['betas'])\n",
    "optimizer_PM = torch.optim.Adam(\n",
    "    params=projPM.parameters(), lr=params['lr'], betas=params['betas'])\n",
    "optimizer_DM = torch.optim.Adam(\n",
    "    params=discM.parameters(), lr=params['lr'], betas=params['betas'])\n",
    "optimizer_DP = torch.optim.Adam(\n",
    "    params=discP.parameters(), lr=params['lr'], betas=params['betas'])\n",
    "\n",
    "def linear_lambda_rule(epoch):\n",
    "    lr_l = 1.0 - max(0, epoch - params['total_epochs']) / float(params['total_epochs'] + 1)#n_epochs_decay + 1)\n",
    "    return lr_l\n",
    "\n",
    "scheduler_MP = torch.optim.lr_scheduler.LambdaLR(optimizer_MP, lr_lambda=linear_lambda_rule)\n",
    "scheduler_PM = torch.optim.lr_scheduler.LambdaLR(optimizer_PM, lr_lambda=linear_lambda_rule)\n",
    "scheduler_DM = torch.optim.lr_scheduler.LambdaLR(optimizer_DM, lr_lambda=linear_lambda_rule)\n",
    "scheduler_DP = torch.optim.lr_scheduler.LambdaLR(optimizer_DP, lr_lambda=linear_lambda_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9905814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(data, device='cpu'):\n",
    "    if isinstance(data, list):\n",
    "        return [item.to(device) for item in data]\n",
    "    else:\n",
    "        return data.to(device)\n",
    "    \n",
    "lambda_M = 10\n",
    "lambda_P = 10\n",
    "lambda_Idt = 0.5\n",
    "\n",
    "\n",
    "def forward( \n",
    "    models,\n",
    "    batch, # raw data from dataloader\n",
    "    logger, # neptune run\n",
    "    mode # 'train'/'val'\n",
    "): # -> loss\n",
    "    \n",
    "    projPM, projMP, discM, discP = models\n",
    "\n",
    "    batch = move_to_device(batch, device)\n",
    "    \n",
    "    meshes = batch[0:4]\n",
    "    points = batch[4]\n",
    "    face_index = batch[5]\n",
    "    \n",
    "    mout = mesh_model.model.forward_features(meshes).detach()\n",
    "    pout = point_model.model.forward_features(points).detach()\n",
    "    \n",
    "    real_meshes = torch.gather(mout, 2, face_index.unsqueeze(1).expand((-1, 512, -1)))\n",
    "    real_points = pout\n",
    "    \n",
    "    fake_meshes = projPM(real_points)\n",
    "    fake_points = projMP(real_meshes)\n",
    "    \n",
    "    rec_meshes = projPM(fake_points)\n",
    "    rec_points = projMP(fake_meshes)\n",
    "    \n",
    "    disc_loss_M = LOSS_D(discM(real_meshes), discM(fake_meshes)) * 0.5\n",
    "    disc_loss_P = LOSS_D(discP(real_meshes), discP(fake_meshes)) * 0.5\n",
    "    \n",
    "    fool_disc_loss_M2P = LOSS_G(discP(fake_points))\n",
    "    fool_disc_loss_P2M = LOSS_G(discM(fake_meshes))\n",
    "    \n",
    "    cycle_loss_M = criterion_L1(rec_meshes, real_meshes) * lambda_M\n",
    "    cycle_loss_P = criterion_L1(rec_points, real_points) * lambda_P\n",
    "\n",
    "    id_loss_M2P = criterion_L1(projPM(real_meshes), real_meshes) * lambda_M * lambda_Idt\n",
    "    id_loss_P2M = criterion_L1(projMP(real_points), real_points) * lambda_P * lambda_Idt\n",
    "    \n",
    "    rec_M = criterion_L1(real_meshes, fake_meshes)\n",
    "    rec_P = criterion_L1(real_points, fake_points)\n",
    "    \n",
    "    gen_loss = fool_disc_loss_M2P + fool_disc_loss_P2M + cycle_loss_M + cycle_loss_P + id_loss_M2P + id_loss_P2M\n",
    "        \n",
    "    return {\n",
    "        'loss': gen_loss + disc_loss_M + disc_loss_P + rec_M + rec_P,\n",
    "        'rec_loss': rec_M + rec_P,\n",
    "        'gen_loss': gen_loss,\n",
    "        'discM': disc_loss_M,\n",
    "        'discP': disc_loss_P\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7353c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def log_dict(dict_loss, logger, mode):\n",
    "    for k, v in dict_loss.items():\n",
    "        logger[mode + '/' + k].log(v)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_val_loss(models, loader, logger, forward):\n",
    "    (model.train() for model in models)\n",
    "    progress_bar = tqdm(loader, leave=True, position=0, desc='Validation')\n",
    "    dict_loss = defaultdict(int)\n",
    "    cur_iter = 0\n",
    "    \n",
    "    \n",
    "    for batch in progress_bar:        \n",
    "        loss = forward(models, batch, logger, 'val')\n",
    "\n",
    "        for k, v in loss.items():\n",
    "            dict_loss[k] += v.item()\n",
    "        cur_iter += 1\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': dict_loss['loss'] / cur_iter\n",
    "        })\n",
    "\n",
    "    return {k: v / cur_iter for k, v in dict_loss.items()}\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    models: torch.nn.Module,\n",
    "    params: Dict[str, Any],\n",
    "    logger: Any,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    optimizers: torch.optim.Optimizer,\n",
    "    schedulers: Any,\n",
    "    forward: Callable[[torch.nn.Module, Any, Any, str], Dict[str, torch.Tensor]],\n",
    "):\n",
    "    '''\n",
    "    :param model: torch.nn.Module model\n",
    "    :param params: experiment parameters\n",
    "    :param logger: logger, neptune instance in our case\n",
    "    :param train_loader: train loader\n",
    "    :param val_loader: val loader\n",
    "    :param optimizer: optimizer\n",
    "    :param scheduler: scheduler\n",
    "    :param forward: forward(model, batch, logger, 'val'/'train') -> loss\n",
    "    loss is dict with Tensors, .backward() is called on 'loss' key, other keys are only logged\n",
    "    '''\n",
    "    exp_id = logger['sys/id'].fetch()\n",
    "    save_dir = Path('{}/{}'.format(params['weights_root'], exp_id))\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, params['total_epochs'] + 1):\n",
    "        progress_bar = tqdm(train_loader, leave=True, position=0)\n",
    "\n",
    "        dict_loss = defaultdict(int)\n",
    "        cur_iter = 0\n",
    "\n",
    "\n",
    "        (model.train() for model in models)\n",
    "        for batch in progress_bar:\n",
    "            (optimizer.zero_grad() for optimizer in optimizers)\n",
    "\n",
    "            loss = forward(models, batch, logger, 'train')\n",
    "            \n",
    "            for k, v in loss.items():\n",
    "                dict_loss[k] += v.item()\n",
    "        \n",
    "            cur_iter += 1\n",
    "\n",
    "            loss['loss'].backward()\n",
    "            (optimizer.step() for optimizer in optimizers)\n",
    "            (scheduler.step() for scheduler in schedulers)\n",
    "            progress_bar.set_postfix({\n",
    "                'Epoch': epoch,\n",
    "                'Loss': dict_loss['loss'] / cur_iter\n",
    "            }) \n",
    "\n",
    "        dict_loss = {k: v / cur_iter for k, v in dict_loss.items()}\n",
    "\n",
    "        log_dict(dict_loss, logger, 'train')\n",
    "        \n",
    "        val_dict_loss = calc_val_loss(models, val_loader, logger, forward)\n",
    "        log_dict(val_dict_loss, logger, 'val')\n",
    "\n",
    "        if val_dict_loss['loss'] < best_val_loss:\n",
    "            best_val_loss = val_dict_loss['loss']\n",
    "            (torch.save(model.state_dict(), save_dir / f'{i}_{epoch}val_best.pt')\n",
    "             for i, model in enumerate(models))\n",
    "            \n",
    "                    \n",
    "        if epoch % params['save_every'] == 0:\n",
    "            (torch.save(model.state_dict(), save_dir / f'{i}_{epoch}epoch.pt') for i, model in enumerate(models))\n",
    "    \n",
    "    if params['total_epochs'] % params['save_every'] != 0:\n",
    "        (torch.save(model.state_dict(), save_dir / f'{i}_{epoch}epoch.pt') for i, model in enumerate(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d742ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:28<00:00,  6.63it/s, Epoch=1, Loss=76.7]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:29<00:00,  8.52it/s, Loss=76.3]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:29<00:00,  6.60it/s, Epoch=2, Loss=76.7]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:28<00:00,  8.62it/s, Loss=76.3]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:28<00:00,  6.62it/s, Epoch=3, Loss=76.7]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:28<00:00,  8.59it/s, Loss=76.2]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:28<00:00,  6.62it/s, Epoch=4, Loss=76.7]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:28<00:00,  8.58it/s, Loss=76.2]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:29<00:00,  6.61it/s, Epoch=5, Loss=76.7]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:28<00:00,  8.58it/s, Loss=76.3]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:28<00:00,  6.61it/s, Epoch=6, Loss=76.7]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:28<00:00,  8.53it/s, Loss=76.3]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:29<00:00,  6.61it/s, Epoch=7, Loss=76.7]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:28<00:00,  8.56it/s, Loss=76.2]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:29<00:00,  6.60it/s, Epoch=8, Loss=76.7]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:28<00:00,  8.58it/s, Loss=76.3]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:29<00:00,  6.61it/s, Epoch=9, Loss=76.7]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:28<00:00,  8.53it/s, Loss=76.2]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:29<00:00,  6.60it/s, Epoch=10, Loss=76.7]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:28<00:00,  8.55it/s, Loss=76.2]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:29<00:00,  6.60it/s, Epoch=11, Loss=76.7]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:28<00:00,  8.60it/s, Loss=76.2]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463/985 [01:10<01:19,  6.55it/s, Epoch=12, Loss=76.6]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-97602ae9cf73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mprojPM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojMP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m      \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-fb46349226fd>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(models, params, logger, train_loader, val_loader, optimizers, schedulers, forward)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mdict_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mcur_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    (projPM, projMP, discM, discP),\n",
    "     params,\n",
    "     logger,\n",
    "     train_loader,\n",
    "     test_loader,\n",
    "     (optimizer_MP, optimizer_PM, optimizer_DM, optimizer_DP),\n",
    "     (scheduler_MP, scheduler_PM, scheduler_DM, scheduler_DP),\n",
    "     forward\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52836902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
